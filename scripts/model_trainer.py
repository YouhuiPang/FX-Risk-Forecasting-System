# scripts/model_trainer.py
import pandas as pd
import numpy as np
import os
import pickle
from sklearn.metrics import classification_report, confusion_matrix, f1_score, make_scorer
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import EditedNearestNeighbours
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import StratifiedKFold


def clean_data(X):
    print(f"æ¸…æ´—æ•°æ®å‰ï¼Œè¡Œæ•°: {len(X)}, åˆ—æ•°: {len(X.columns)}")
    X = X.replace([np.inf, -np.inf], np.nan)
    max_float = np.finfo(np.float64).max
    numeric_cols = X.select_dtypes(include=[np.number]).columns
    X[numeric_cols] = X[numeric_cols].where(X[numeric_cols].abs() <= max_float, np.nan)
    key_cols = [col for col in X.columns if col not in ['fed_funds_rate', 'treasury_10y', 'cpi', 'usd_index']]
    X = X.dropna(subset=key_cols)
    print(f"æ¸…æ´—æ•°æ®åï¼Œè¡Œæ•°: {len(X)}, åˆ—æ•°: {len(X.columns)}")
    return X


def add_macro_anomaly_features(X, macro_cols):
    print(f"æ·»åŠ å®è§‚å¼‚å¸¸ç‰¹å¾ï¼Œåˆå§‹åˆ—: {list(X.columns)}")
    X_new = X.copy()
    for col in macro_cols:
        if col not in X_new.columns:
            print(f"å®è§‚æŒ‡æ ‡ {col} ä¸å­˜åœ¨ï¼Œè·³è¿‡...")
            continue
        mean_val = X_new[col].mean()
        std_val = X_new[col].std()
        if std_val == 0 or pd.isna(std_val):
            z_scores = pd.Series(0, index=X_new.index)
            print(f"å®è§‚æŒ‡æ ‡ {col} æ ‡å‡†å·®ä¸º0ï¼Œæœªè®¡ç®—Zåˆ†æ•°")
        else:
            z_scores = (X_new[col] - mean_val) / std_val
            X_new[f'{col}_anomaly_weighted'] = X_new[col] * (z_scores.abs() > 2).astype(int) * z_scores.abs()
            if 'rolling_std_20' in X_new.columns:
                X_new[f'{col}_x_vol'] = X_new[col] * X_new['rolling_std_20']
    print(f"æ·»åŠ å®è§‚å¼‚å¸¸ç‰¹å¾åï¼Œåˆ—æ•°: {len(X_new.columns)}")
    return X_new


def custom_risk_score(y_true, y_pred):
    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)
    recall_high = report.get("2", {}).get("recall", 0)
    recall_medium = report.get("1", {}).get("recall", 0)
    return 0.5 * recall_high + 0.5 * recall_medium

scorer = make_scorer(custom_risk_score, greater_is_better=True)



def train_and_save_model(pair, features_path):
    print(f"\nè®­ç»ƒæ¨¡å‹ï¼š{pair}ï¼ˆå¤šæ ‡ç­¾ risk_levelï¼‰")

    # === åŠ è½½å¹¶é¢„å¤„ç†æ•°æ® ===
    df = pd.read_csv(features_path, parse_dates=["date"]).sort_values("date")
    if len(df) > 2:
        df = df.iloc[:-2]
    X = df[[col for col in df.columns if col not in ["date", "future_min_return", "is_high_risk", "risk_level"]]]
    y = df["risk_level"]
    if y.nunique() < 2:
        print(" æ ‡ç­¾ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹"); return

    # åˆ’åˆ†æ•°æ®
    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42)

    # æ¸…æ´— & ç‰¹å¾å¤„ç†
    for name, X_set, y_set in [("train", X_train, y_train), ("val", X_val, y_val), ("test", X_test, y_test)]:
        X_set = clean_data(X_set)
        X_set = add_macro_anomaly_features(X_set, ['fed_funds_rate', 'treasury_10y', 'cpi', 'usd_index'])
        if name == "train": X_train, y_train = X_set, y_set.loc[X_set.index]
        elif name == "val": X_val, y_val = X_set, y_set.loc[X_set.index]
        else: X_test, y_test = X_set, y_set.loc[X_set.index]

    # å»é™¤å¸¸æ•°ç‰¹å¾
    constant_cols = X_train.columns[X_train.nunique() <= 1]
    if len(constant_cols) > 0:
        X_train = X_train.drop(columns=constant_cols)
        X_val = X_val.drop(columns=constant_cols)
        X_test = X_test.drop(columns=constant_cols)

    # ç‰¹å¾é€‰æ‹©
    selector = SelectKBest(score_func=f_classif, k=min(15, X_train.shape[1]))
    selector.fit(X_train, y_train)
    selected_features = X_train.columns[selector.get_support()].tolist()
    print(f"\n é€‰æ‹©çš„ç‰¹å¾: {selected_features}")
    X_train = X_train[selected_features]
    X_val = X_val[selected_features]
    X_test = X_test[selected_features]

    # é‡é‡‡æ ·ï¼šSMOTE
    smote = SMOTE(sampling_strategy={1: 300, 2: 300}, random_state=42)
    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
    print(f" SMOTE å: {np.bincount(y_resampled.astype(int))}")

    # ç±»åˆ«æƒé‡
    class_weights = {0: 1.0, 1: 4, 2: 2}
    sample_weight = y_resampled.map(class_weights).values

    # ========== Grid Search ==========
    param_grid = {
        "learning_rate": [0.1],
        "max_depth": [8],
        "n_estimators": [700],
        "reg_alpha": [10],
        "reg_lambda": [10],
        "subsample": [0.8],
        "colsample_bytree": [1.0]
    }

    scorer = make_scorer(f1_score, average='weighted')

    base_model = XGBClassifier(
        objective="multi:softprob",
        num_class=3,
        eval_metric="mlogloss",
        verbosity=0,
        random_state=42
    )

    grid = GridSearchCV(
        estimator=base_model,
        param_grid=param_grid,
        scoring=scorer,
        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
        n_jobs=-1,
        verbose=1
    )
    grid.fit(X_resampled, y_resampled, sample_weight=sample_weight)
    best_model = grid.best_estimator_

    print(f"\n æœ€ä½³å‚æ•°: {grid.best_params_}")
    print(f"æœ€ä½³åŠ æƒå¾—åˆ†: {grid.best_score_:.4f}")

    cv_scores = []
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    for train_idx, val_idx in skf.split(X_resampled, y_resampled):
        X_tr, X_val_ = X_resampled.iloc[train_idx], X_resampled.iloc[val_idx]
        y_tr, y_val_ = y_resampled.iloc[train_idx], y_resampled.iloc[val_idx]
        sw_tr = sample_weight[train_idx]

        model = XGBClassifier(**grid.best_params_, objective="multi:softprob", num_class=3)
        model.fit(X_tr, y_tr, sample_weight=sw_tr)

        y_pred_ = model.predict(X_val_)
        score = f1_score(y_val_, y_pred_, average="weighted")
        cv_scores.append(score)

    print(f"æ‰‹åŠ¨åŠ æƒ 5æŠ˜ F1-weighted: {np.mean(cv_scores):.3f} Â± {np.std(cv_scores):.3f}")

    # ========== Final Evaluation ==========
    y_train_pred = best_model.predict(X_train)
    print("\nğŸ“Š è®­ç»ƒé›†åˆ†ç±»ç»“æœï¼š")
    print(confusion_matrix(y_train, y_train_pred))
    print(classification_report(y_train, y_train_pred, target_names=["Low", "Medium", "High"]))

    y_val_pred = best_model.predict(X_val)
    print("\nğŸ“Š éªŒè¯é›†åˆ†ç±»ç»“æœï¼š")
    print(confusion_matrix(y_val, y_val_pred))
    print(classification_report(y_val, y_val_pred, target_names=["Low", "Medium", "High"]))

    y_test_pred = best_model.predict(X_test)
    print("\nğŸ“Š æµ‹è¯•é›†åˆ†ç±»ç»“æœï¼š")
    print(confusion_matrix(y_test, y_test_pred))
    print(classification_report(y_test, y_test_pred, target_names=["Low", "Medium", "High"]))

    # ========== ä¿å­˜æ¨¡å‹ ==========
    root = os.path.dirname(os.path.dirname(__file__))
    model_dir = os.path.join(root, "models")
    os.makedirs(model_dir, exist_ok=True)

    model_path = os.path.join(model_dir, f"risk_model_{pair.lower().replace('/', '_')}_xgb_multi.pkl")
    with open(model_path, "wb") as f:
        pickle.dump(best_model, f)

    feature_path = os.path.join(model_dir, f"selected_features_{pair.lower().replace('/', '_')}.pkl")
    with open(feature_path, "wb") as f:
        pickle.dump(selected_features, f)

    print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜ï¼š{model_path}")
    print(f"âœ… ç‰¹å¾ä¿å­˜è‡³ï¼š{feature_path}")
    return best_model, selected_features


def load_model_and_features(pair):
    """åŠ è½½æ¨¡å‹å’Œç‰¹å¾"""
    root = os.path.dirname(os.path.dirname(__file__))
    model_dir = os.path.join(root, "models")
    model_path = os.path.join(model_dir, f"risk_model_{pair.lower().replace('/', '_')}_xgb_new.pkl")
    features_path = os.path.join(model_dir, f"selected_features_{pair.lower().replace('/', '_')}.pkl")

    with open(model_path, "rb") as f:
        model = pickle.load(f)
    with open(features_path, "rb") as f:
        selected_features = pickle.load(f)

    return model, selected_features


if __name__ == "__main__":
    pair = "USD/CNY"
    root = os.path.dirname(os.path.dirname(__file__))
    features_path = os.path.join(root, "data", f"features_{pair.lower().replace('/', '_')}.csv")

    # è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹
    train_and_save_model(pair, features_path)