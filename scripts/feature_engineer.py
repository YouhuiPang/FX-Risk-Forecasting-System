# scripts/feature_engineer.py
import pandas as pd
import os
import numpy as np
from scipy.stats import genpareto

def load_exchange_data(exchange_df):
    df = exchange_df.copy()
    df["close_price"] = pd.to_numeric(df["close_price"], errors="coerce")
    df["return"] = df["close_price"].pct_change()
    df = df.dropna(subset=["return"])
    return df


def add_technical_indicators(df):
    df["rolling_mean_5"] = df["close_price"].rolling(5, min_periods=1).mean()
    df["rolling_mean_20"] = df["close_price"].rolling(20, min_periods=1).mean()
    df["price_above_ma"] = df["close_price"] > df["rolling_mean_20"]
    df["momentum_10"] = df["close_price"].diff(10)
    df["rolling_std_5"] = df["close_price"].rolling(5, min_periods=1).std()
    df["rolling_std_20"] = df["close_price"].rolling(20, min_periods=1).std()
    df["bollinger_upper"] = df["rolling_mean_20"] + 2 * df["rolling_std_20"]
    df["bollinger_lower"] = df["rolling_mean_20"] - 2 * df["rolling_std_20"]
    df["high_low_range_5"] = df["close_price"].rolling(5, min_periods=1).max() - df["close_price"].rolling(5, min_periods=1).min()
    rolling_max = df["close_price"].rolling(10, min_periods=1).max()
    df["drawdown_10"] = (df["close_price"] - rolling_max) / rolling_max
    df["ma_diff"] = df["rolling_mean_5"] - df["rolling_mean_20"]
    df["return_slope_5"] = df["return"].rolling(5).mean()
    df["drawdown_speed"] = (df["close_price"] - df["close_price"].rolling(5).max()) / 5
    df["bollinger_width"] = df["bollinger_upper"] - df["bollinger_lower"]
    df["volatility_ratio"] = df["rolling_std_5"] / (df["rolling_std_20"] + 1e-6)
    df["range_return_ratio"] = df["high_low_range_5"] / (df["return"].rolling(5).std() + 1e-6)

    delta = df["close_price"].diff()
    gain = (delta.where(delta > 0, 0)).rolling(14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
    rs = gain / (loss + 1e-9)  # é¿å…é™¤0é”™è¯¯
    df["rsi_14"] = 100 - (100 / (1 + rs))
    df["rsi_slope"] = df["rsi_14"].diff()


    return df

def load_macro_data(macro_df):
    """åŠ è½½å¹¶é¢„å¤„ç†å®è§‚æ•°æ®"""
    df = macro_df.copy()
    df = df.sort_values("date").ffill().bfill()  # å¡«å……ç¼ºå¤±å€¼
    return df


def merge_features(exchange_df, macro_df, pair, pot_quantile=0.75, evt_quantile=0.95):
    """åˆå¹¶ç‰¹å¾å¹¶ç”Ÿæˆå¤šæ ‡ç­¾ EVT é£é™©ç­‰çº§ï¼ˆ0=ä½, 1=ä¸­, 2=é«˜ï¼‰"""
    exchange_df = exchange_df.dropna(subset=["date"])
    macro_df = macro_df.dropna(subset=["date"])

    df = pd.merge_asof(
        exchange_df.sort_values("date"),
        macro_df.sort_values("date"),
        on="date",
        direction="backward"
    )

    # Step 1: ç”Ÿæˆæœªæ¥3æ—¥æœ€å°æ”¶ç›Š
    df["future_min_return"] = df["return"].rolling(3, min_periods=1).min().shift(-2)

    # Step 2: EVT æ‹Ÿåˆé«˜é£é™©é˜ˆå€¼
    down_returns = df["future_min_return"][df["future_min_return"] < 0].abs()
    pot_threshold = down_returns.quantile(pot_quantile)
    excesses = down_returns[down_returns > pot_threshold] - pot_threshold

    c, loc, scale = genpareto.fit(excesses)
    n, nu = len(down_returns), len(excesses)
    extreme_threshold = pot_threshold + (scale / c) * ((n / nu * (1 - evt_quantile)) ** (-c) - 1)

    # Step 3: ä¸­é£é™©ç•Œé™ï¼ˆç»éªŒåˆ†ä½æ•°ï¼‰
    moderate_threshold = down_returns.quantile(0.75)

    # Step 4: å¤šæ ‡ç­¾æ‰“åˆ†ï¼ˆä¸Šæ¶¨ä¹Ÿä½œä¸ºä½é£é™©ï¼‰
    def classify_risk(row):
        x = row["future_min_return"]
        if pd.isna(x):
            return np.nan
        if x >= 0:
            return 0  # Low Riskï¼ˆä¸Šæ¶¨æˆ–ä¸è·Œï¼‰
        x = abs(x)
        if x > extreme_threshold:
            return 2  # High Risk
        elif x > moderate_threshold:
            return 1  # Medium Risk
        else:
            return 0  # Low Riskï¼ˆå°å¹…ä¸‹è·Œï¼‰

    df["risk_level"] = df.apply(classify_risk, axis=1)

    # Debug è¾“å‡ºï¼šé˜ˆå€¼ + æ ‡ç­¾åˆ†å¸ƒ
    print(f"ğŸ“‰ EVTæç«¯é£é™©é˜ˆå€¼ï¼ˆq={evt_quantile:.3f}ï¼‰ï¼š{extreme_threshold:.4%}")
    print(f"ğŸ“Š ä¸­é£é™©ç•Œé™ï¼ˆ70%åˆ†ä½ï¼‰ï¼š{moderate_threshold:.4%}")
    print("âœ… å„ç±»é£é™©æ ‡ç­¾åˆ†å¸ƒ:")
    print(df["risk_level"].value_counts(dropna=False))

    # Step 5: æ¸…æ´—æ•°æ®ï¼Œä¿ç•™æœ€åä¸¤å¤©ç”¨äºé¢„æµ‹
    if len(df) > 2:
        df_train = df.iloc[:-2].dropna(subset=["future_min_return", "risk_level"])
        df_tail = df.iloc[-2:]
        df_final = pd.concat([df_train, df_tail], axis=0)
    else:
        df_final = df.dropna(subset=["future_min_return", "risk_level"])

    print(f"ğŸ“¦ åˆå¹¶åæ•°æ®é‡: {len(df_final)} è¡Œ")
    return df_final



def update_features(pair, features_path, exchange_df, macro_df):
    """æ›´æ–°ç‰¹å¾æ•°æ®"""
    # åŠ è½½ç°æœ‰ç‰¹å¾æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if os.path.exists(features_path):
        try:
            features_df = pd.read_csv(features_path, parse_dates=["date"])
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç°æœ‰ç‰¹å¾æ•°æ®å¤±è´¥: {e}")
            features_df = pd.DataFrame()
    else:
        features_df = pd.DataFrame()

    # ç”Ÿæˆæ–°ç‰¹å¾
    exchange_df = load_exchange_data(exchange_df)
    exchange_df = add_technical_indicators(exchange_df)
    macro_df = load_macro_data(macro_df)
    new_features = merge_features(exchange_df, macro_df, pair)

    # è¿½åŠ æ–°ç‰¹å¾å¹¶å»é‡
    if not new_features.empty:
        print(f"ğŸ“¥ ç”Ÿæˆæ–°ç‰¹å¾æ•°æ®ï¼Œè¡Œæ•°: {len(new_features)}")
        features_df = pd.concat([features_df, new_features], ignore_index=True)
        features_df = features_df.drop_duplicates(subset=['date'], keep='last')
        features_df = features_df.sort_values("date")
        print(f"ğŸ“Š åˆå¹¶åæ•°æ®è¡Œæ•°: {len(features_df)}")
        features_df.to_csv(features_path, index=False)
        print(f"âœ… æ›´æ–°ç‰¹å¾æ•°æ®ä¿å­˜è‡³: {features_path}")
    else:
        print("âš ï¸ æœªç”Ÿæˆæ–°ç‰¹å¾æ•°æ®ï¼Œä½¿ç”¨ç°æœ‰æ•°æ®ã€‚")

    return features_df

if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å¼ï¼šåŠ è½½ç°æœ‰æ•°æ®å¹¶ç”Ÿæˆç‰¹å¾
    pair = "USD/CNY"
    root = os.path.dirname(os.path.dirname(__file__))
    exchange_data_path = os.path.join(root, "data", "usd_cny.csv")
    macro_data_path = os.path.join(root, "data", "macro.csv")
    features_path = os.path.join(root, "data", f"features_{pair.lower().replace('/', '_')}.csv")

    exchange_df = pd.read_csv(exchange_data_path, parse_dates=["date"])
    macro_df = pd.read_csv(macro_data_path, parse_dates=["date"])
    df_final = update_features(pair, features_path, exchange_df, macro_df)